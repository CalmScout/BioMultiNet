{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading biological data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the adaptation of the part of the [MultilayerNexus](https://github.com/cirillodavide/MultilayerNexus.git) project for downloading various biological datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp download_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from itertools import combinations\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import rdflib\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import gzip\n",
    "import shutil\n",
    "import cobra\n",
    "import urllib.request\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BioGRIG interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/multilayer_network/raw/BIOGRID-ORGANISM-4.4.231.tab3.zip: 169MB [00:11, 15.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction graph created and saved as JSON at ../data/multilayer_network/processed/BioGRID_interactions.23-09-2024.json\n"
     ]
    }
   ],
   "source": [
    "protein_interaction_techniques = {\n",
    "    \"Affinity Capture-Luminescence\",\n",
    "    \"Affinity Capture-MS\",\n",
    "    \"Affinity Capture-Western\",\n",
    "    \"Biochemical Activity\",\n",
    "    \"Co-crystal Structure\",\n",
    "    \"Co-fractionation\",\n",
    "    \"Co-localization\",\n",
    "    \"Co-purification\",\n",
    "    \"Far Western\",\n",
    "    \"FRET\",\n",
    "    \"PCA\",\n",
    "    \"Protein-peptide\",\n",
    "    \"Reconstituted Complex\",\n",
    "    \"Two-hybrid\",\n",
    "}\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "   file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_file(url, path, file_size):\n",
    "    ensure_dir(path)\n",
    "    chunk_size = 1024  # 1KB\n",
    "    with requests.get(url, stream=True) as r, open(path, 'wb') as f, tqdm(\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            total=file_size,\n",
    "            desc=str(path)\n",
    "    ) as progress:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            datasize = f.write(chunk)\n",
    "            progress.update(datasize)\n",
    "\n",
    "\n",
    "date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "release = '4.4.231'\n",
    "base_path = Path('../data/multilayer_network/raw')\n",
    "zip_path = base_path / f'BIOGRID-ORGANISM-{release}.tab3.zip'\n",
    "specific_file = f'BIOGRID-ORGANISM-Homo_sapiens-{release}.tab3.txt'\n",
    "txt_path = base_path / specific_file\n",
    "output_json_path = Path(f'../data/multilayer_network/processed/BioGRID_interactions.{date}.json')\n",
    "\n",
    "url = f'https://downloads.thebiogrid.org/Download/BioGRID/Release-Archive/BIOGRID-{release}/BIOGRID-ORGANISM-{release}.tab3.zip'\n",
    "\n",
    "file_size = int(requests.head(url).headers.get('content-length', 0))\n",
    "\n",
    "download_file(url, zip_path, file_size)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extract(specific_file, base_path)\n",
    "os.remove(zip_path)\n",
    "\n",
    "nodes = {}\n",
    "edges = []\n",
    "\n",
    "with open(txt_path, 'r') as file:\n",
    "    next(file)  # Skip header\n",
    "    for line in file:\n",
    "        cols = line.strip().split('\\t')\n",
    "        if cols[15] == cols[16] == \"9606\" and cols[11] in protein_interaction_techniques:\n",
    "            int1, int2 = cols[7], cols[8]\n",
    "            nodes[int1] = {\"label\": int1, \"type\": \"protein\"}\n",
    "            nodes[int2] = {\"label\": int2, \"type\": \"protein\"}\n",
    "            edges.append({\"source\": int1, \"target\": int2, \"attributes\": {\"Interaction Type\": cols[12], \"Experimental System\": cols[11]}})\n",
    "\n",
    "nodes_list = [{\"id\": k, **v} for k, v in nodes.items()]\n",
    "final_structure = {\"nodes\": nodes_list, \"edges\": edges}\n",
    "\n",
    "ensure_dir(output_json_path)\n",
    "\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(final_structure, json_file, indent=4)\n",
    "\n",
    "print(f\"Interaction graph created and saved as JSON at {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. KEGG drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/multilayer_network/raw/br08310.json: 675kB [00:03, 207kB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created at ../data/multilayer_network/processed/KEGG_drugs.23-09-2024.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def download_file_with_progress(url, filename):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        file_size = int(r.headers.get('content-length', 0))\n",
    "        chunk_size = max(4096, file_size // 100)\n",
    "        with open(filename, 'wb') as file_out, tqdm(\n",
    "            total=file_size, unit='B', unit_scale=True, desc=str(filename)\n",
    "        ) as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                file_out.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "release = 'br08310'\n",
    "url = f'https://www.genome.jp/kegg-bin/download_htext?htext={release}.keg&format=json&filedir='\n",
    "\n",
    "data_dir = Path('../data/multilayer_network/raw')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "download_file_with_progress(url, data_dir / f'{release}.json')\n",
    "\n",
    "def extract_drugs_and_receptors(node, drug_to_receptors):\n",
    "    if 'children' in node:\n",
    "        if 'name' in node and '[' in node['name']:\n",
    "            receptor_name = re.sub(r'\\s*\\(.*?\\)\\s*', '', node['name'].split('[')[0].strip()).replace('*', '')\n",
    "            for drug in node.get('children', []):\n",
    "                drug_id, _, drug_name = drug['name'].partition(' ')\n",
    "                drug_name = drug_name.lstrip()\n",
    "                drug_to_receptors.setdefault(drug_id, {'receptors': [], 'drug_name': drug_name}).get('receptors').append(receptor_name)\n",
    "        else:\n",
    "            for child in node.get('children', []):\n",
    "                extract_drugs_and_receptors(child, drug_to_receptors)\n",
    "\n",
    "drug_to_receptors = {}\n",
    "extract_drugs_and_receptors(json.loads((data_dir / f'{release}.json').read_text()), drug_to_receptors)\n",
    "\n",
    "output = {\n",
    "    \"nodes\": [\n",
    "        {\"id\": receptor, \"label\": receptor, \"type\": \"protein\"} for receptor in {receptor for info in drug_to_receptors.values() for receptor in info['receptors']}\n",
    "    ],\n",
    "    \"edges\": [\n",
    "        {\"source\": src, \"target\": tgt, \"attributes\": {\"Drug ID\": drug_id, \"Drug Name\": info['drug_name']}}\n",
    "        for drug_id, info in drug_to_receptors.items() for src, tgt in combinations(set(info['receptors']), 2)\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_path = Path('../data/multilayer_network/processed') / f'KEGG_drugs.{date}.json'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "output_path.write_text(json.dumps(output, indent=4))\n",
    "\n",
    "print(f\"JSON file created at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MONDO diseases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL address `http://purl.obolibrary.org/obo/mondo.owl` contains a 215 MB ontology, however the cell below doesn't create any processed dataset. Changed schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total_size_in_bytes = int(r.headers.get('content-length', 0))\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, desc=\"Downloading ontology\")\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                progress_bar.update(file.write(chunk))\n",
    "        progress_bar.close()\n",
    "\n",
    "def extract_mondo2omim(g):\n",
    "    rdfl = rdflib.Namespace('http://www.w3.org/2000/01/rdf-schema#')\n",
    "    owl = rdflib.Namespace('http://www.w3.org/2002/07/owl#')\n",
    "    oboInOwl = rdflib.Namespace('http://www.geneontology.org/formats/oboInOwl#')\n",
    "    mondo2omim = {}\n",
    "    mondo2descr = {}\n",
    "\n",
    "    for subj in tqdm(g.subjects(), desc=\"Extracting subjects\"):\n",
    "        for i in g.triples((subj, owl['annotatedSource'], None)):\n",
    "            mondo = i[2]\n",
    "            mondoID = mondo.toPython().rsplit('/', 1)[-1]\n",
    "            for j in g.triples((mondo, rdfl['label'], None)):\n",
    "                descr = j[2].toPython()\n",
    "                mondo2descr[mondoID] = descr\n",
    "            for w in g.triples((mondo, oboInOwl['hasDbXref'], None)):\n",
    "                if w[2].startswith(rdflib.Literal('OMIM:')):\n",
    "                    omim = w[2].toPython()\n",
    "                    mondo2omim[mondoID] = omim\n",
    "    return mondo2omim, mondo2descr\n",
    "\n",
    "def fetch_genes(url):\n",
    "    try:\n",
    "        df = pd.read_csv(url, sep='\\t', header=0).dropna(subset=['evidence'])\n",
    "        df = df[df['subject_taxon_label'].str.contains('Homo sapiens', na=False)]\n",
    "        genes = df[df['evidence'].str.contains('ECO:0000220')].subject_label.unique().tolist()\n",
    "        return genes\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch genes from URL: {url} due to {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_json(mondo2genes, mondo2descr, output_file):\n",
    "    nodes = [{\"id\": gene, \"label\": gene, \"type\": \"gene\"} for gene_list in mondo2genes.values() for gene in gene_list]\n",
    "    edges = [{\"source\": gene_pair[0], \"target\": gene_pair[1], \"attributes\": {\"MONDO Identifier\": mondo, \"Description\": mondo2descr[mondo]}} \n",
    "             for mondo, genes in mondo2genes.items() for gene_pair in combinations(genes, 2)]\n",
    "\n",
    "    final_json = {\"nodes\": list({v['id']:v for v in nodes}.values()), \"edges\": edges}\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(final_json, json_file, indent=4)\n",
    "\n",
    "url = 'http://purl.obolibrary.org/obo/mondo.owl'\n",
    "filename = '../data/multilayer_network/raw/mondo.owl'\n",
    "output_path = Path(\"../data/multilayer_network/processed/MoNDO_diseases.\" + datetime.now().strftime(\"%d-%m-%Y\") + \".json\")\n",
    "\n",
    "\n",
    "download_file(url, filename)\n",
    "g = rdflib.Graph()\n",
    "# g.load(filename)\n",
    "g.parse(filename)\n",
    "mondo2omim, mondo2descr = extract_mondo2omim(g)\n",
    "    \n",
    "inputs = [f\"https://solr.monarchinitiative.org/solr/golr/select/?defType=edismax&qt=standard&indent=on&wt=csv&rows=100000&start=0&fl=subject,subject_label,subject_taxon,subject_taxon_label,object,object_label,relation,relation_label,evidence,evidence_label,source,is_defined_by,qualifier&facet=true&facet.mincount=1&facet.sort=count&json.nl=arrarr&facet.limit=25&facet.method=enum&csv.encapsulator=%22&csv.separator=%09&csv.header=true&csv.mv.separator=%7C&fq=subject_category:%22gene%22&fq=object_category:%22disease%22&fq=object_closure:%22{mondo.replace('_', ':')}%22&facet.field=subject_taxon_label&q=*:*\" for mondo in mondo2omim.keys()]\n",
    "with multiprocessing.Pool() as pool:\n",
    "    genes_list = list(tqdm(pool.imap(fetch_genes, inputs), total=len(inputs), desc=\"Fetching genes\"))\n",
    "    \n",
    "mondo2genes = {mondo: genes for mondo, genes in zip(mondo2omim.keys(), genes_list) if genes}\n",
    "generate_json(mondo2genes, mondo2descr, output_path)\n",
    "print(f\"Generated JSON file at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reactome pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "# Download Uniprot id mapping\n",
    "url = \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/by_organism/HUMAN_9606_idmapping.dat.gz\"\n",
    "response = requests.get(url)\n",
    "filename = 'data/multilayer_network/raw/HUMAN_9606_idmapping.dat.gz'\n",
    "outname = 'data/multilayer_network/raw/HUMAN_9606_idmapping.dat'\n",
    "with open(filename, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "with gzip.open(filename, 'rb') as f_in:\n",
    "    with open(outname, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "os.remove(filename)\n",
    "mapping_dict = {}\n",
    "with open(outname, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        uni_id, identifier_type, value = parts[0], parts[1], ' '.join(parts[2:])\n",
    "        if identifier_type == 'Gene_Name':\n",
    "            mapping_dict[uni_id] = value\n",
    "\n",
    "# Download Reactome lowest levels\n",
    "url = 'https://reactome.org/download/current/UniProt2Reactome.txt'\n",
    "response = requests.get(url)\n",
    "filename = 'data/multilayer_network/raw/UniProt2Reactome.txt'\n",
    "with open(filename, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "nodes = []\n",
    "edges = []\n",
    "\n",
    "reactome = {}\n",
    "descr_dict = {}\n",
    "with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) < 6:\n",
    "            continue  # Skip incomplete lines\n",
    "        prot, pathway, descr, ec, taxo = parts[0], parts[1], parts[3], parts[4], parts[5]\n",
    "        descr_dict[pathway] = descr\n",
    "        if taxo != \"Homo sapiens\":\n",
    "            continue\n",
    "        # Skip certain evidence codes if needed\n",
    "        # if ec not in [\"EXP\", \"IDA\", \"IPI\", \"IMP\", \"IGI\", \"IEP\"]:\n",
    "        #    continue\n",
    "        if pathway not in reactome:\n",
    "            reactome[pathway] = []\n",
    "        if prot not in reactome[pathway]:\n",
    "            try:\n",
    "                gene = mapping_dict[prot]\n",
    "            except:\n",
    "                gene = prot\n",
    "            reactome[pathway].append(gene)\n",
    "        if gene not in [node['id'] for node in nodes]:\n",
    "            nodes.append({\n",
    "                \"id\": gene,\n",
    "                \"label\": gene,\n",
    "                \"type\": \"protein\"\n",
    "            })\n",
    "\n",
    "for k,v in reactome.items():\n",
    "    if len(v) > 1:\n",
    "        for gene_pair in combinations(v, 2):\n",
    "            edges.append({\n",
    "                \"source\": gene_pair[0],\n",
    "                \"target\": gene_pair[1],\n",
    "                \"attributes\": {\n",
    "                    \"Reactome Identifier\": k,\n",
    "                    \"Description\": descr_dict[k]\n",
    "                }\n",
    "            })\n",
    "\n",
    "final_json = {\"nodes\": nodes, \"edges\": edges}\n",
    "file_name = \"data/multilayer_network/processed/Reactome_pathways.\" + now.strftime(\"%d-%m-%Y\") + \".json\"\n",
    "with open(file_name, 'w') as json_file:\n",
    "    json.dump(final_json, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recon3D metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current timestamp for naming the output file later\n",
    "now = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# Function to download and decompress the model file\n",
    "def download_and_decompress(url, output_path):\n",
    "    with urllib.request.urlopen(url) as response, open(output_path, 'wb') as out_file:\n",
    "        out_file.write(gzip.decompress(response.read()))\n",
    "\n",
    "# Define URLs and file paths\n",
    "model_url = 'http://bigg.ucsd.edu/static/models/Recon3D.xml.gz'\n",
    "model_file_path = 'data/multilayer_network/raw/Recon3D.xml'\n",
    "metabolites_file_path = 'src/metabolites_to_prune.txt'\n",
    "\n",
    "# Download and import the Recon3D model\n",
    "download_and_decompress(model_url, model_file_path)\n",
    "model = cobra.io.read_sbml_model(model_file_path)\n",
    "\n",
    "# Import the metabolites to prune\n",
    "with open(metabolites_file_path) as f:\n",
    "    remove_metabolites = [line.split(' ')[0].strip() for line in f]\n",
    "\n",
    "# Function to process metabolites and genes\n",
    "def process_metabolites_and_genes(model, remove_metabolites):\n",
    "    products_dict, reactants_dict, metabo_name_dict = defaultdict(list), defaultdict(list), {}\n",
    "    gene_pattern, metabo_pattern = r'__[clmerginx]$', r'__\\d+__\\d+$'\n",
    "\n",
    "    for reaction in model.reactions:\n",
    "        for gene in filter(lambda g: g.id.split('_')[0] != \"0\", reaction.genes):\n",
    "            gene_id = gene.id.split('_')[0]\n",
    "            for metabolite_group, metabolites in (('products', reaction.products), ('reactants', reaction.reactants)):\n",
    "                for metabolite in metabolites:\n",
    "                    metabo_id = re.sub(gene_pattern, '', metabolite.id)\n",
    "                    metabo_name_dict[metabo_id] = metabolite.name\n",
    "                    if gene_id not in (products_dict if metabolite_group == 'products' else reactants_dict)[metabo_id]:\n",
    "                        (products_dict if metabolite_group == 'products' else reactants_dict)[metabo_id].append(gene.name)\n",
    "\n",
    "    metabolites_dict = defaultdict(list)\n",
    "    for metabo_id, genes in {**products_dict, **reactants_dict}.items():\n",
    "        if metabo_id not in remove_metabolites:\n",
    "            metabolites_dict[re.sub(metabo_pattern, '', metabo_id)].extend(set(genes))\n",
    "\n",
    "    return {k: list(set(v)) for k, v in metabolites_dict.items()}, metabo_name_dict\n",
    "\n",
    "metabolites_dict, metabo_name_dict = process_metabolites_and_genes(model, remove_metabolites)\n",
    "\n",
    "# Preparing data for JSON output\n",
    "nodes = [{\"id\": gene, \"label\": gene, \"type\": \"protein\"} for gene in set(gene for genes in metabolites_dict.values() for gene in genes)]\n",
    "edges = [{\n",
    "    \"source\": gene_pair[0],\n",
    "    \"target\": gene_pair[1],\n",
    "    \"attributes\": {\"Metabolite\": metabo, \"Descriptive name\": metabo_name_dict[metabo]}\n",
    "} for metabo, genes in metabolites_dict.items() for gene_pair in combinations(genes, 2)]\n",
    "\n",
    "# Saving the graph to a JSON file\n",
    "output_file_path = f'data/multilayer_network/processed/Recon3D_metabolites.{now}.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    json.dump({\"nodes\": nodes, \"edges\": edges}, f, indent=4)\n",
    "\n",
    "print(f\"Graph saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
