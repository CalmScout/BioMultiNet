[
  {
    "objectID": "cmmd.html",
    "href": "cmmd.html",
    "title": "CmmD: Continual Multiplex network Module Detector",
    "section": "",
    "text": "Python implementation of the algorithm originally proposed in “The multilayer community structure of medulloblastoma” by Iker Núñez-Carpintero et. al..\nImplementation relies on MolTi-DREAM for communities detection problem.",
    "crumbs": [
      "CmmD: Continual Multiplex network Module Detector"
    ]
  },
  {
    "objectID": "cmmd.html#import-libraries-and-define-paths",
    "href": "cmmd.html#import-libraries-and-define-paths",
    "title": "CmmD: Continual Multiplex network Module Detector",
    "section": "0. Import libraries and define paths",
    "text": "0. Import libraries and define paths\nAuxiliary function to measure the execution time:\n\n\ntime_it\n\n time_it (func)",
    "crumbs": [
      "CmmD: Continual Multiplex network Module Detector"
    ]
  },
  {
    "objectID": "cmmd.html#define-function-for-community-detection",
    "href": "cmmd.html#define-function-for-community-detection",
    "title": "CmmD: Continual Multiplex network Module Detector",
    "section": "1. Define function for community detection",
    "text": "1. Define function for community detection\n\n\ncommunities_detection\n\n communities_detection (input_layers:list[str]=None, gamma_min:float=None,\n                        gamma_max:float=None, gamma_step:float=None,\n                        path_to_communities:str=None, method:str='molti')\n\n\n\n\ncontinual_multiplex_analysis\n\n continual_multiplex_analysis (nodelist:list[str]=None,\n                               path_to_communities:str=None,\n                               distmethod:str='hamming', n_jobs:int=1)\n\n\n\n\ncmmd\n\n cmmd (nodelist:list[str]|None=None, input_layers:list[str]=None,\n       gamma_min:float=None, gamma_max:float=None, gamma_step:float=None,\n       distmethod:str='hamming', method:str='molti', n_jobs:int=1,\n       path_to_communities:str=None)\n\nCompute CmmD multilayer community trajectory analysis for a set of given networks.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnodelist\nlist[str] | None\nNone\nA list with the unique nodes that we want to appear in the final output. If not given,all nodes of the multiplex will be in the final output (nodelist= NULL)\n\n\ninput_layers\nlist\nNone\nA vector of strings containing the paths where the different network layers are locatedin the system. Networks should be a two column file representing the edges of the graph.\n\n\ngamma_min\nfloat\nNone\nThe first gamma resolution parameter to use in the different MolTi’s analysis\n\n\ngamma_max\nfloat\nNone\nThe last gamma resolution parameter to use in the different MolTi’s analysis.\n\n\ngamma_step\nfloat\nNone\nThe gamma_step of the resolution parameter to use.\n\n\ndistmethod\nstr\nhamming\nA distance method metric to use to compute the trajectories. Defaults to “hamming” for hammingdistance, but accepts any other metric supplied by scipy.spatial.distance.pdist.\n\n\nmethod\nstr\nmolti\n\n\n\nn_jobs\nint\n1\nThe number of n_jobs to use for the computation of the distance matrix. Defaults to 1.\n\n\npath_to_communities\nstr\nNone\nThe path to save Molti’s output files. Defaults to “Output/”.\n\n\nReturns\nA dictionary containing the following keys:\n\ngene_community_matrix: A matrix where the rows correspond to the different genes, and the columns to the different community structures. The values of the matrix are the cluster to which the gene belongs in the corresponding community structure.l_constant: A dictionary where the keys are the different community structures, and the values are the list of genes that belong to that community structure.distance_matrix: A matrix with the hamming distances between all pairs of genes.\n\n\n\n\n# prefix = \"../data/bsc_baseline_tiny/\"\nprefix = \"../data/synthetic/\"\n\ninput_layers = [prefix + x for x in os.listdir(prefix) if x.endswith(\".csv\")]\n\nfor l in input_layers:\n    assert Path(l).exists()\n\ncmmd_output = cmmd(nodelist = None,\n     input_layers = input_layers,\n     gamma_min = 0,\n     gamma_max = 30,\n     gamma_step = 0.5,\n     path_to_communities = \"../out/communities/\",\n     distmethod = \"hamming\",\n     n_jobs = 6)\n\nClean folder that contains the gerated files of the communities:\n\nCLEAN = False\nfolder_path = Path(\"../out/communities/\")\n\nif CLEAN:\n    for item in os.listdir(folder_path):\n        shutil.rmtree(os.path.join(folder_path, item)) if os.path.isdir(os.path.join(folder_path, item)) else os.remove(os.path.join(folder_path, item))\n\nSave object to the disk:\n\n# # save object to file\n# path_save = Path(\"../out/cmmd_pickle/\")\n# assert path_save.exists()\n\n# # pickle the object\n# with open(path_save / \"cmmd_output.pkl\", 'wb') as handle:\n#     pickle.dump(cmmd_output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n# def test_cmmd(cmmd:dict, path_save:pathlib.Path|str=Path(\"../out/cmmd_pickle/cmmd_output.pkl\")) -&gt; None:\n#     # load the cmmd object\n#     with open(path_save, 'rb') as handle:\n#         cmmd_old = pickle.load(handle)\n#     assert (cmmd[\"distance_matrix\"] == cmmd_old[\"distance_matrix\"]).all()\n#     assert (cmmd[\"gene_community_matrix\"] == cmmd_old[\"gene_community_matrix\"]).all()\n#     assert cmmd[\"l_constant\"] == cmmd_old[\"l_constant\"]\n\nConfirm new and old objects generate the same output:\n\ncmmd_output.keys()\n\n\ncmmd_output[\"distance_matrix\"]\n\n\nfor key in cmmd_output.keys():\n    print(key)\n    print(cmmd_output[key])\n    print(\"*\" * 20)\n\n\ncmmd_output[\"distance_matrix\"].shape\n\n\ndm = cmmd_output[\"distance_matrix\"]\ndm\n\n\nnp.unique(dm)\n\n\nsns.histplot(cmmd_output[\"distance_matrix\"].flatten(), stat=\"density\");\n\n\ngcm = cmmd_output[\"gene_community_matrix\"]\ngcm\n\n\ngcm.shape\n\n\nl_constant = cmmd_output[\"l_constant\"]\ntype(l_constant)\n\n\nlen(l_constant.keys())\n\n\ncmmd_output.keys()",
    "crumbs": [
      "CmmD: Continual Multiplex network Module Detector"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BioMultiNet",
    "section": "",
    "text": "Multilayer networks as defined by Kivelä et al. (2014) generalize graphs to capture the rich network data often associated with complex systems, allowing us to study a broad range of phenomena using a common representation, using the same multilayer tools and methods. Formally, a multilayer network M is defined as a quadruple \\(M = (V_M, E_M, V, L)\\), where the sequence \\(L = (L_a)_{a=1}^d\\) defines sets \\(L_a\\) of elementary layers, the set \\(V\\) defines the nodes of the network, the node-layers are \\(V_M \\subseteq  V \\times  L_1 \\times  ... \\times  L_d\\), and the edges \\(E_M \\subseteq  V_M \\times  V_M\\) are defined between node-layers. Put simply, a node-layer is an association of a node \\(v \\in  V\\) with a layer \\(\\in  L \\times ...\\times  L\\) of dimensionality \\(d\\), nodes can exist on an arbitrary number of layers, and edges can connect node-layers within layers and across arbitrary pairs of layers, which can differ in an arbitrary number of dimensions. The dimensions \\(1, 2, ..., d\\) are called the aspects of the network (e.g., a two-aspect transport network could have time as its first aspect and transport mode as its second aspect)(description from pymnet package).",
    "crumbs": [
      "BioMultiNet"
    ]
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "BioMultiNet",
    "section": "Roadmap",
    "text": "Roadmap\n\nCover code by tests\nAdd functionality to generate synthetic networks\nPort implementation of CmmD algorithm from R to Python\nProfile and optimize code (MNHACK24)\nExplore alternatives to MolTi clustering\nReimplement Louvian clustering in Python, add additional community detection algorithms\nPyG integration",
    "crumbs": [
      "BioMultiNet"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "BioMultiNet",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall BioMultiNet in Development mode\n# make sure BioMultiNet package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to BioMultiNet\n$ nbdev_prepare",
    "crumbs": [
      "BioMultiNet"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "BioMultiNet",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/CalmScout/BioMultiNet.git\nor from conda\n$ conda install -c CalmScout BioMultiNet\nor from pypi\n$ pip install BioMultiNet\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "BioMultiNet"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "BioMultiNet",
    "section": "How to use",
    "text": "How to use\nExamples of utilization will be here:\n\n1+1\n\n2",
    "crumbs": [
      "BioMultiNet"
    ]
  },
  {
    "objectID": "explore_pymnet.html",
    "href": "explore_pymnet.html",
    "title": "Exploration of pymnet capabilities",
    "section": "",
    "text": "Additional text can be added here and it will be reflected in the documentation.\n\nfrom pymnet import *\n\nnet_social = MultiplexNetwork(couplings=\"categorical\", fullyInterconnected=False)\nnet_social[\"Alice\", \"Bob\", \"Friends\"] = 1\nnet_social[\"Alice\", \"Carol\", \"Friends\"] = 1\nnet_social[\"Bob\", \"Carol\", \"Friends\"] = 1\nnet_social[\"Alice\", \"Bob\", \"Married\"] = 1\n\nfig_social = draw(net_social, layout=\"circular\", layerPadding=0.2, defaultLayerLabelLoc=(0.9,0.9))\n\n\n\n\n\n\n\n\nTime required to generate different networks:\n\nimport pymnet\nnet = pymnet.er(10, 10**5*[0.1])\n\n\nnet = pymnet.er(10**5,10*[10**-5])",
    "crumbs": [
      "Exploration of `pymnet` capabilities"
    ]
  },
  {
    "objectID": "explore_pymnet.html#overview",
    "href": "explore_pymnet.html#overview",
    "title": "Exploration of pymnet capabilities",
    "section": "",
    "text": "Additional text can be added here and it will be reflected in the documentation.\n\nfrom pymnet import *\n\nnet_social = MultiplexNetwork(couplings=\"categorical\", fullyInterconnected=False)\nnet_social[\"Alice\", \"Bob\", \"Friends\"] = 1\nnet_social[\"Alice\", \"Carol\", \"Friends\"] = 1\nnet_social[\"Bob\", \"Carol\", \"Friends\"] = 1\nnet_social[\"Alice\", \"Bob\", \"Married\"] = 1\n\nfig_social = draw(net_social, layout=\"circular\", layerPadding=0.2, defaultLayerLabelLoc=(0.9,0.9))\n\n\n\n\n\n\n\n\nTime required to generate different networks:\n\nimport pymnet\nnet = pymnet.er(10, 10**5*[0.1])\n\n\nnet = pymnet.er(10**5,10*[10**-5])",
    "crumbs": [
      "Exploration of `pymnet` capabilities"
    ]
  },
  {
    "objectID": "explore_pymnet.html#network-types",
    "href": "explore_pymnet.html#network-types",
    "title": "Exploration of pymnet capabilities",
    "section": "2. Network types",
    "text": "2. Network types\n\nMonoplex networks\nMonoplex networks have aspect value equals to 0.\n\nnet = pymnet.MultilayerNetwork(aspects=0)\n\n\nnet.add_node(1)\nnet.add_node(2)\n\n\nlist(net)\n\n[1, 2]\n\n\n\nnet[1].deg()\n\n0\n\n\nAdd edge:\n\nnet[1, 2] = 1\n\n\nnet[1, 3] = 1\nlist(net)\n\n[1, 2, 3]\n\n\n\nnet[1, 2], net[2, 1], net[1, 1]\n\n(1, 1, 0)\n\n\n\nlist(net[1])\n\n[2, 3]\n\n\n\nnet[1, 3] = 0\nlist(net[1])\n\n[2]\n\n\n\nnet[1, 3] = 2\nnet[1].deg()\n\n2\n\n\n\nlist(net[1])\n\n[2, 3]\n\n\n\nnet[1].strength()\n\n3\n\n\n\ndirnet = pymnet.MultilayerNetwork(aspects=0, directed=True)\n\ndirnet[1, 2] = 1\n\n\ndirnet[1, 2], dirnet[2, 1]\n\n(1, 0)\n\n\n\nnet[2, 1]\n\n1\n\n\n\n\nMultilayer networks\n\nmnet = pymnet.MultilayerNetwork(aspects=1)\n\n\nmnet.add_node(1)\nmnet.add_layer(\"a\")\n\n\nmnet[1, \"a\"].deg()\n\n0\n\n\n\nmnet[1, 2, \"a\", \"b\"] = 1\n\n\nmnet[1, \"a\"][2, \"b\"] = 1\n\n\nlist(mnet[1, \"a\"])\n\n[(2, 'b')]\n\n\n\nlist(mnet[1, \"b\"])\n\n[]\n\n\n\nlist(mnet)\n\n[1, 2]\n\n\nExamples of the multilayer net with several aspects:\n\nmnet2 = pymnet.MultilayerNetwork(aspects=2)\nmnet2[1, 2, \"a\" ,\"b\", \"x\" ,\"y\"] = 1\nmnet2[1, \"a\", \"x\"][2, \"b\", \"y\"]\n\n1\n\n\n\nlist(mnet2)\n\n[1, 2]\n\n\n\nlist(mnet2[1, \"a\", \"x\"])\n\n[(2, 'b', 'y')]\n\n\n\nmnet2.get_layers()\n\n{'a', 'b'}\n\n\n\nmnet2.add_layer(\"c\", 1)\nmnet2.add_layer(\"z\", 2)\n\n\nmnet2.get_layers()\n\n{'a', 'b', 'c'}\n\n\n\n\nMultiplex networks\n\nmplex = pymnet.MultiplexNetwork(couplings=\"none\")\n\n\nmplex[1, \"a\"][2, \"a\"] = 1\n\nCan access the intra-layer networks as follows:\n\nmplex.A[\"a\"][1, 2]\n\n1\n\n\n\nmplex.A[\"a\"][1, 3] = 1\n\n\ncnet = pymnet.MultiplexNetwork(couplings=\"categorical\")\ncnet.add_node(1)\ncnet.add_layer(\"a\")\ncnet.add_layer(\"b\")\ncnet[1, 1, \"a\", \"b\"]\n\n1.0\n\n\n\nonet = pymnet.MultiplexNetwork(couplings=\"ordinal\")\nonet.add_node(\"node\")\nonet.add_layer(1)\nonet.add_layer(2)\nonet.add_layer(3)\nonet[\"node\", \"node\", 1, 2]\n\n1.0\n\n\n\nonet[\"node\", \"node\", 1, 3]\n\n0\n\n\n\ncnet = pymnet.MultiplexNetwork(couplings=(\"categorical\", 10))\ncnet.add_node(1)\ncnet.add_layer(\"a\")\ncnet.add_layer(\"b\")\ncnet[1, 1, \"a\", \"b\"]\n\n10\n\n\n\nconet = pymnet.MultiplexNetwork(couplings=[\"categorical\", \"ordinal\"])\nconet.add_node(\"node\")\nconet.add_layer(\"a\", 1)\nconet.add_layer(\"b\", 1)\nconet.add_layer(1, 2)\nconet.add_layer(2, 2)\nconet.add_layer(3, 2)\nconet[\"node\", \"node\", \"a\", \"a\", 1, 2]\n\n1.0\n\n\n\nconet.A[(\"a\", 1)][\"node\", \"node2\"] = 1",
    "crumbs": [
      "Exploration of `pymnet` capabilities"
    ]
  },
  {
    "objectID": "explore_pymnet.html#visualization-of-networks",
    "href": "explore_pymnet.html#visualization-of-networks",
    "title": "Exploration of pymnet capabilities",
    "section": "3. Visualization of networks",
    "text": "3. Visualization of networks\n\nimport random\nrandom.seed(42)\nimport numpy as np\nnp.random.seed(42)\n\n\nfrom pymnet import *\nnet = models.er_multilayer(5, 2, 0.2)\nfig = draw(net, layout=\"circular\", layerPadding=0.2, defaultLayerLabelLoc=(0.9,0.9))\n\n\n\n\n\n\n\n\n\nfig.savefig(\"er_multilayer.png\")\n\n\nfig = draw(er(10, 3*[0.5]), layout=\"spring\")\n\n\n\n\n\n\n\n\n\nfig = draw(er(10, 3*[0.3]),\n           layout=\"circular\",\n           layershape=\"circle\",\n           nodeColorDict={(0,0):\"r\", (1,0):\"r\", (0,1):\"r\"},\n           layerLabelRule={},\n           nodeLabelRule={},\n           nodeSizeRule={\"rule\":\"degree\", \"propscale\":0.05}\n           )\n\n\n\n\n\n\n\n\n\nimport requests\ndataset = \"bkfrat.dat\"\nfraternity_dataset_url = f\"http://vlado.fmf.uni-lj.si/pub/networks/data/ucinet/{dataset}\"\nres = requests.get(fraternity_dataset_url)\nwith open(dataset, \"wb\") as f:\n    f.write(res.content)\n\n\nnet = read_ucinet(dataset, couplings=\"none\")\nnet = transforms.threshold(net, 4)\nfig = draw(net,\n           layout=\"spring\",\n           layerColorRule={},\n           defaultLayerColor=\"silver\",\n           nodeLabelRule={},\n           edgeColorRule={\"rule\":\"edgeweight\", \"colormap\":\"viridis\", \"scaleby\":0.1},\n           defaultLayerLabelLoc=(0.9,0.9)\n           )",
    "crumbs": [
      "Exploration of `pymnet` capabilities"
    ]
  },
  {
    "objectID": "explore_pymnet.html#isomorphysms-and-automorphisms",
    "href": "explore_pymnet.html#isomorphysms-and-automorphisms",
    "title": "Exploration of pymnet capabilities",
    "section": "4. Isomorphysms and automorphisms",
    "text": "4. Isomorphysms and automorphisms\n\nSetup\n\nfrom pymnet import *\nnet_social = MultiplexNetwork(couplings=\"categorical\", fullyInterconnected=False)\nnet_social[\"Alice\", \"Bob\", \"Friends\"] = 1\nnet_social[\"Alice\", \"Carol\", \"Friends\"] = 1\nnet_social[\"Bob\", \"Carol\", \"Friends\"] = 1\nnet_social[\"Alice\", \"Bob\", \"Married\"] = 1\n# net_social[\"John\", \"Bob\", \"Bussiness\"] = 1\nfig_social = draw(net_social, layout=\"circular\", layerPadding=0.2, defaultLayerLabelLoc=(0.9,0.9))\n\n\n\n\n\n\n\n\n\nnet_transport = MultiplexNetwork(couplings=\"categorical\", fullyInterconnected=False)\nnet_transport[\"Helsinki\", \"Turku\", \"Train\"] = 1\nnet_transport[\"Helsinki\", \"Tampere\", \"Train\"] = 1\nnet_transport[\"Turku\", \"Tampere\", \"Train\"] = 1\nnet_transport[\"Helsinki\", \"Turku\", \"Ferry\"] = 1\nfig_transport = draw(net_transport, layout=\"circular\", layerPadding=0.2, defaultLayerLabelLoc=(0.9,0.9))\n\n\n\n\n\n\n\n\n\n\nIsomorphisms\n\n# node isomorphism\nis_isomorphic(net_social, net_transport, allowed_aspects=[0])\n\nFalse\n\n\n\n# layer isomorphism\nis_isomorphic(net_transport, net_social, allowed_aspects=[1])\n\nFalse\n\n\n\n# node-layer isomorphism\nis_isomorphic(net_transport, net_social, allowed_aspects=[0, 1])\n\nTrue\n\n\n\nget_isomorphism(net_transport, net_social)\n\n[{'Helsinki': 'Bob', 'Tampere': 'Carol', 'Turku': 'Alice'},\n {'Ferry': 'Married', 'Train': 'Friends'}]\n\n\n\nnet_social_complete_invariant = get_complete_invariant(net_social)\nnet_transport_complete_invariant = get_complete_invariant(net_transport)\nprint(net_social_complete_invariant, net_transport_complete_invariant)\n\n((((), 0),), &lt;bliss_bind.NamedGraph object&gt;) ((((), 0),), &lt;bliss_bind.NamedGraph object&gt;)\n\n\n\n\nAutomorphisms\n\n# node isomorphism\nget_automorphism_generators(net_social, allowed_aspects=[0])\n\n[[{'Bob': 'Alice', 'Alice': 'Bob'}, {}]]\n\n\n\n# layer isomorphism\nget_automorphism_generators(net_social, allowed_aspects=[1])\n\n[]\n\n\n\n# node-layer isomorphism\nget_automorphism_generators(net_social, allowed_aspects=[0, 1])\n\n[[{'Bob': 'Alice', 'Alice': 'Bob'}, {}]]\n\n\n\nnet = MultilayerNetwork(aspects=1)\nnet[1, \"A\"][2, \"A\"] = 1\nnet[2, \"A\"][3, \"A\"] = 1\nnet[3, \"B\"][2, \"B\"] = 1\nnet[2, \"B\"][1, \"B\"] = 1\nnet[3, \"A\"][1, \"B\"] = 1\nfig = draw(net, layerPadding=0.2)\n\n\n\n\n\n\n\n\n\nget_automorphism_generators(net, allowed_aspects=[0])\n\n[]\n\n\n\nget_automorphism_generators(net, allowed_aspects=[1])\n\n[]\n\n\n\nget_automorphism_generators(net, allowed_aspects=[0, 1])\n\n[[{1: 3, 3: 1}, {'A': 'B', 'B': 'A'}]]",
    "crumbs": [
      "Exploration of `pymnet` capabilities"
    ]
  },
  {
    "objectID": "explore_pymnet.html#using-networkx-functions",
    "href": "explore_pymnet.html#using-networkx-functions",
    "title": "Exploration of pymnet capabilities",
    "section": "5. Using NetworkX functions",
    "text": "5. Using NetworkX functions\n\nfrom pymnet import nx\n\n\nnet = nx.karate_club_graph()\n\n\ntype(net)\n\npymnet.net.MultilayerNetwork\n\n\n\nnet.aspects\n\n0\n\n\n\nimport random\nrandom.seed(42)\n\n\nimport pymnet\n{name: nx.number_connected_components(layer) for name, layer in pymnet.er(1000, 3*[0.005]).A.items()}\n\n{0: 10, 1: 9, 2: 5}",
    "crumbs": [
      "Exploration of `pymnet` capabilities"
    ]
  },
  {
    "objectID": "community_detection.html#load-multilayer-network-to-pymnet",
    "href": "community_detection.html#load-multilayer-network-to-pymnet",
    "title": "Community detection",
    "section": "1. Load multilayer network to pymnet",
    "text": "1. Load multilayer network to pymnet\nLet’s explore data structure"
  },
  {
    "objectID": "community_detection.html#community-detection-algorithms",
    "href": "community_detection.html#community-detection-algorithms",
    "title": "Community detection",
    "section": "2. Community detection algorithms",
    "text": "2. Community detection algorithms\nThere are different algorithms for community detection in a multilayer networks."
  },
  {
    "objectID": "code_profiling.html",
    "href": "code_profiling.html",
    "title": "Community detection code profiling",
    "section": "",
    "text": "Notebook generates synthetic datasets of different configurations (different number of layers and elements per layer) and explores the performance of code (execution time and memory consumption) for each of selected configurations. Initial version relies on MolTi implementation for the communities detection problem, but pipeline can be adapted to benchmarking of different communities detection algorithms like InfoMap.",
    "crumbs": [
      "Community detection code profiling"
    ]
  },
  {
    "objectID": "code_profiling.html#import-dependencies",
    "href": "code_profiling.html#import-dependencies",
    "title": "Community detection code profiling",
    "section": "0. Import dependencies",
    "text": "0. Import dependencies\n\n# add parent directory to path so we can import utils\nimport sys\nimport os\nimport shutil\nsys.path.append('../')\nimport time\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom pathlib import Path\nimport numpy as np\nfrom BioMultiNet.utils import *\nfrom BioMultiNet.cmmd import *\nimport networkx as nx\nfrom tqdm.notebook import tqdm\n\n\npath_to_benchmark = Path(\"../data/benchmark/\")\npath_to_benchmark.mkdir(exist_ok=True)",
    "crumbs": [
      "Community detection code profiling"
    ]
  },
  {
    "objectID": "code_profiling.html#generate-synthetic-datasets",
    "href": "code_profiling.html#generate-synthetic-datasets",
    "title": "Community detection code profiling",
    "section": "1. Generate synthetic datasets",
    "text": "1. Generate synthetic datasets\n\n1.1 Scenario 1\nIncrease number of layers, all layers have the same number of nodes in them (200).\n\nn_layers = np.arange(1, 101, 1)\nn = 200\np = 0.2\n\n\npath_to = path_to_benchmark / \"scenario-1\"\npath_to.mkdir(exist_ok=True)\n\n\nobfuscate = True\nlabel_edges = \"\"\n\nfor n_layer in tqdm(n_layers):\n    # create directory for the dataset with current number of layers\n    path_to = path_to_benchmark / f\"scenario-1/{n_layer}\"\n    path_to.mkdir(exist_ok=True)\n\n    # populate created directory with synthetic layers\n    node_labels_pool = []\n    for n_layer_idx in range(n_layer):\n        G = create_and_save_random_graph(generator=nx.erdos_renyi_graph,\n                                        label_edges=label_edges,\n                                        path_to=path_to / f\"{n_layer_idx}.csv\",\n                                        n=n,\n                                        p=p,\n                                        directed=False,\n                                        obfuscate=obfuscate,\n                                        node_labels_pool=node_labels_pool)\n        node_labels_pool.extend(list(G.nodes()))",
    "crumbs": [
      "Community detection code profiling"
    ]
  },
  {
    "objectID": "code_profiling.html#benchmarking",
    "href": "code_profiling.html#benchmarking",
    "title": "Community detection code profiling",
    "section": "2. Benchmarking",
    "text": "2. Benchmarking\nFor the selected scenario let’s compute community detection problem, log consumed resources and viaualize them.\n\nd = {}\n\nprefix_ = path_to_benchmark / \"scenario-1\"\n\nfor k in tqdm(os.listdir(prefix_)):\n    # select the folder with the mentioned number of layers\n    prefix = str(prefix_) + \"/\" + k\n    # list of the filenames - input layers\n    input_layers = [prefix + \"/\" + x for x in os.listdir(prefix) if x.endswith(\".csv\")]\n\n    for l in input_layers:\n        assert Path(l).exists()\n\n    start = time.time()\n    cmmd_output = cmmd(nodelist = None,\n        input_layers = input_layers,\n        gamma_min = 0,\n        gamma_max = 30,\n        gamma_step = 0.5,\n        path_to_communities = \"../out/communities/\",\n        distmethod = \"hamming\",\n        n_jobs = 6)\n    \n    duration = time.time() - start\n\n    # clean up\n    shutil.rmtree(\"../out/communities/\")\n\n    d[k] = duration\n\nprint(d)\n\n\n\n\n{'31': 6.770324945449829, '75': 14.484583854675293, '42': 8.959571838378906, '15': 3.7124531269073486, '71': 13.9744873046875, '3': 1.1011457443237305, '78': 15.205758571624756, '24': 5.371524095535278, '84': 16.141170501708984, '100': 19.349994897842407, '72': 14.28057074546814, '85': 16.635345220565796, '59': 11.813063621520996, '12': 3.14388370513916, '88': 17.228288412094116, '90': 17.65051031112671, '66': 13.965642213821411, '8': 2.285508394241333, '17': 4.0732581615448, '4': 1.3713326454162598, '38': 8.058760643005371, '20': 4.781110763549805, '93': 18.834080934524536, '18': 4.254984140396118, '21': 4.830830335617065, '5': 1.65264892578125, '70': 13.980457305908203, '65': 12.718465566635132, '73': 14.824580669403076, '22': 5.000579357147217, '9': 2.51029634475708, '6': 1.818345546722412, '53': 11.076967477798462, '2': 0.8684158325195312, '91': 17.708168745040894, '98': 18.87727403640747, '55': 11.234635591506958, '50': 10.009679794311523, '49': 10.039834260940552, '94': 18.309519290924072, '33': 7.08008074760437, '99': 19.352848052978516, '76': 15.50121808052063, '54': 10.795984029769897, '60': 11.950036764144897, '69': 13.361162662506104, '68': 13.784982204437256, '89': 17.226262092590332, '26': 5.859835386276245, '23': 5.183246850967407, '77': 15.258048057556152, '44': 9.00645661354065, '61': 12.308953523635864, '37': 8.04716944694519, '7': 1.9662771224975586, '28': 5.9620726108551025, '32': 6.768187761306763, '39': 8.059011697769165, '25': 5.441216945648193, '56': 11.154169797897339, '64': 12.184103012084961, '74': 14.449753284454346, '29': 6.344885587692261, '82': 16.61689043045044, '11': 2.9028053283691406, '16': 3.9098587036132812, '48': 9.772667646408081, '80': 15.921027898788452, '95': 18.184608221054077, '81': 15.95603346824646, '62': 12.219666004180908, '86': 16.82018208503723, '35': 7.4861719608306885, '51': 10.61707329750061, '57': 11.506405115127563, '83': 16.195647716522217, '43': 9.08676028251648, '41': 8.651748895645142, '40': 8.447950601577759, '36': 7.587794303894043, '63': 12.859253168106079, '19': 4.550904273986816, '67': 13.500339031219482, '79': 15.75006628036499, '58': 11.457986116409302, '13': 2.803053379058838, '1': 0.5027480125427246, '14': 3.0981554985046387, '87': 15.07165265083313, '10': 2.4170913696289062, '46': 8.43606424331665, '27': 5.302222013473511, '47': 8.52686333656311, '96': 16.940941333770752, '52': 9.4132080078125, '30': 5.892106771469116, '45': 8.464815378189087, '92': 16.185070991516113, '97': 17.319541692733765, '34': 6.687211036682129}\n\n\n\n# sort the dictionary\nsorted_d = dict(sorted(d.items(), key=lambda item: item[1]))\n\n\n# create a dataframe from the dictionary\ndf = pd.DataFrame.from_dict(sorted_d, orient='index')\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'n_layers', 0: 'duration'})\ndf.head()\n\n\n\n\n\n\n\n\nn_layers\nduration\n\n\n\n\n0\n1\n0.502748\n\n\n1\n2\n0.868416\n\n\n2\n3\n1.101146\n\n\n3\n4\n1.371333\n\n\n4\n5\n1.652649\n\n\n\n\n\n\n\n\n# save the dataframe to csv\ndf.to_csv(\"../out/duration.csv\", index=False)\n\n\nwidth = 15\nheight = 8\nmatplotlib.rcParams['figure.figsize'] = [width, height]\n\n\n# Create the plot (using a line plot as an example)\nsns.lineplot(x='n_layers', y='duration', data=df, marker='o')\n\n# Rotate the x-axis labels for a compact look\nplt.xticks(rotation=45)  # Rotate by 45 degrees\n\n# Add labels and title\nplt.xlabel('Number of Layers')\nplt.ylabel('Execution time, s')\nplt.title('Execution time vs Number of Layers')\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Community detection code profiling"
    ]
  },
  {
    "objectID": "generate_data.html",
    "href": "generate_data.html",
    "title": "Dataset generation",
    "section": "",
    "text": "There are several approaches to generate data to start work with multilayer networks:",
    "crumbs": [
      "Dataset generation"
    ]
  },
  {
    "objectID": "generate_data.html#imports-and-paths",
    "href": "generate_data.html#imports-and-paths",
    "title": "Dataset generation",
    "section": "0. Imports and paths",
    "text": "0. Imports and paths\n\npath_data = Path(\"../data\")\npath_data.mkdir(exist_ok=True)",
    "crumbs": [
      "Dataset generation"
    ]
  },
  {
    "objectID": "generate_data.html#generate-synthetic-dataset",
    "href": "generate_data.html#generate-synthetic-dataset",
    "title": "Dataset generation",
    "section": "1. Generate synthetic dataset",
    "text": "1. Generate synthetic dataset\nCreate a small multilayer graph for testing purposes.\n\n\nobfuscate_nodes\n\n obfuscate_nodes (n_nodes:int, node_labels_pool:list[str]=None,\n                  str_len:int=10)\n\nMaps nodes from range [0, n_nodes) to unique random strings of length str_len. All upper case letters and digits are used, starts with a letter. If node_labels_pool is provided, nodes selected from it if len(node_labels_pool) &gt;= n_nodes, otherwise, additional unique nodes are generated.\n\nd = obfuscate_nodes(n_nodes=3)\nd\n\n{0: 'PAZN0O9IIH', 1: 'MNMUZUM48X', 2: 'VDBU4HPMCC'}\n\n\n\nnode_labels_pool = ['TI672Y15ZU', 'S0SM5PZWDN','G54WMQPO11']\nd = obfuscate_nodes(n_nodes=2, node_labels_pool=node_labels_pool)\nd\n\n{0: 'S0SM5PZWDN', 1: 'TI672Y15ZU'}\n\n\n\nd = obfuscate_nodes(n_nodes=5, node_labels_pool=node_labels_pool)\nd\n\n{0: 'JNFJDR2ZFN',\n 1: 'S0SM5PZWDN',\n 2: 'UTG42RLL53',\n 3: 'TI672Y15ZU',\n 4: 'G54WMQPO11'}\n\n\nThe following is a thin wrapper around the random graph generators from networkx package.\n\n\n\ncreate_random_graph\n\n create_random_graph (generator:&lt;built-infunctioncallable&gt;,\n                      obfuscate:bool=True,\n                      node_labels_pool:list[str]=None, **kwargs)\n\n*Thin wrapper around networkx’s random graph generator\nArgs: generator (callable): networkx’s random graph generator **kwargs: passed to generator Example: &gt;&gt;&gt; G = create_random_graph(nx.erdos_renyi_graph, n=10, p=0.6, directed=False)*\n\nnode_labels_pool\n\n['G54WMQPO11', 'S0SM5PZWDN', 'TI672Y15ZU']\n\n\n\nG = create_random_graph(nx.erdos_renyi_graph, node_labels_pool=node_labels_pool, n=5, p=0.6, directed=False)\n\n\nG.nodes()\n\nNodeView(('SBCUX8F7ST', 'S0SM5PZWDN', 'G54WMQPO11', 'TI672Y15ZU', 'JYSAI9KPVS'))\n\n\nCreate random graph and write it to the disk:\n\n\n\ncreate_and_save_random_graph\n\n create_and_save_random_graph (generator:&lt;built-infunctioncallable&gt;,\n                               label_edges:str, path_to:str|pathlib.Path,\n                               obfuscate:bool=True,\n                               node_labels_pool:list[str]=None, **kwargs)\n\n*Creates a random graph and saves it to path_to.\nArgs: generator (callable): networkx’s random graph generator obfuscate (bool, optional): if true, obfuscates node names. Defaults to True. path_to (str|Path, optional): path to save the graph. Defaults to None.\nExample: &gt;&gt;&gt; create_random_graph_and_save(nx.erdos_renyi_graph, n=10, p=0.6, directed=False, path_to=path_data / ‘synthetic’ / ‘1.csv’)*\nCreate three layers. For experiments with CmmD algorithm we need at least several hundreds of nodes to see the difference in the clustering with a variety of \\(\\gamma\\) values:\n\npath_dir_to = path_data / 'synthetic'\npath_dir_to.mkdir(exist_ok=True)\n\nLeaving the label_edge empty allows to follow the required format for MolTi community detection software:\n\nnode_labels_pool = []\n\nn, p, label_edges = 300, 0.2, \"\"\nG_1 = create_and_save_random_graph(nx.erdos_renyi_graph, label_edges, path_dir_to / '1.csv',\n                                   obfuscate=True, node_labels_pool=node_labels_pool, n=n, p=p)\nnode_labels_pool.extend(list(G_1.nodes()))\n\nn, p, label_edges = 500, 0.2, \"\"\nG_2 = create_and_save_random_graph(nx.erdos_renyi_graph, label_edges, path_dir_to / '2.csv',\n                                   obfuscate=True, node_labels_pool=node_labels_pool, n=n, p=p)\nnode_labels_pool.extend(list(G_2.nodes()))\n\nn, p, label_edges = 400, 0.2, \"\"\nG_3 = create_and_save_random_graph(nx.erdos_renyi_graph, label_edges, path_dir_to / '3.csv',\n                                   obfuscate=True, node_labels_pool=node_labels_pool, n=n, p=p)\nnode_labels_pool.extend(list(G_3.nodes()))",
    "crumbs": [
      "Dataset generation"
    ]
  },
  {
    "objectID": "download_data.html",
    "href": "download_data.html",
    "title": "Downloading biological data",
    "section": "",
    "text": "Below is the adaptation of the part of the MultilayerNexus project for downloading various biological datasets.",
    "crumbs": [
      "Downloading biological data"
    ]
  },
  {
    "objectID": "download_data.html#import-required-packages",
    "href": "download_data.html#import-required-packages",
    "title": "Downloading biological data",
    "section": "0. Import required packages",
    "text": "0. Import required packages\n\nimport requests\nimport zipfile\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport json\nfrom pathlib import Path\nimport os\nfrom itertools import combinations\nimport re\nimport pandas as pd\nimport ast\nimport rdflib\nimport multiprocessing\nfrom joblib import Parallel, delayed\nimport gzip\nimport shutil\nimport cobra\nimport urllib.request\nfrom collections import defaultdict",
    "crumbs": [
      "Downloading biological data"
    ]
  },
  {
    "objectID": "download_data.html#biogrig-interactions",
    "href": "download_data.html#biogrig-interactions",
    "title": "Downloading biological data",
    "section": "1. BioGRIG interactions",
    "text": "1. BioGRIG interactions\n\nprotein_interaction_techniques = {\n    \"Affinity Capture-Luminescence\",\n    \"Affinity Capture-MS\",\n    \"Affinity Capture-Western\",\n    \"Biochemical Activity\",\n    \"Co-crystal Structure\",\n    \"Co-fractionation\",\n    \"Co-localization\",\n    \"Co-purification\",\n    \"Far Western\",\n    \"FRET\",\n    \"PCA\",\n    \"Protein-peptide\",\n    \"Reconstituted Complex\",\n    \"Two-hybrid\",\n}\n\ndef ensure_dir(file_path):\n   file_path.parent.mkdir(parents=True, exist_ok=True)\n\ndef download_file(url, path, file_size):\n    ensure_dir(path)\n    chunk_size = 1024  # 1KB\n    with requests.get(url, stream=True) as r, open(path, 'wb') as f, tqdm(\n            unit='B',\n            unit_scale=True,\n            total=file_size,\n            desc=str(path)\n    ) as progress:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            datasize = f.write(chunk)\n            progress.update(datasize)\n\n\ndate = datetime.now().strftime(\"%d-%m-%Y\")\nrelease = '4.4.231'\nbase_path = Path('../data/multilayer_network/raw')\nzip_path = base_path / f'BIOGRID-ORGANISM-{release}.tab3.zip'\nspecific_file = f'BIOGRID-ORGANISM-Homo_sapiens-{release}.tab3.txt'\ntxt_path = base_path / specific_file\noutput_json_path = Path(f'../data/multilayer_network/processed/BioGRID_interactions.{date}.json')\n\nurl = f'https://downloads.thebiogrid.org/Download/BioGRID/Release-Archive/BIOGRID-{release}/BIOGRID-ORGANISM-{release}.tab3.zip'\n\nfile_size = int(requests.head(url).headers.get('content-length', 0))\n\ndownload_file(url, zip_path, file_size)\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extract(specific_file, base_path)\nos.remove(zip_path)\n\nnodes = {}\nedges = []\n\nwith open(txt_path, 'r') as file:\n    next(file)  # Skip header\n    for line in file:\n        cols = line.strip().split('\\t')\n        if cols[15] == cols[16] == \"9606\" and cols[11] in protein_interaction_techniques:\n            int1, int2 = cols[7], cols[8]\n            nodes[int1] = {\"label\": int1, \"type\": \"protein\"}\n            nodes[int2] = {\"label\": int2, \"type\": \"protein\"}\n            edges.append({\"source\": int1, \"target\": int2, \"attributes\": {\"Interaction Type\": cols[12], \"Experimental System\": cols[11]}})\n\nnodes_list = [{\"id\": k, **v} for k, v in nodes.items()]\nfinal_structure = {\"nodes\": nodes_list, \"edges\": edges}\n\nensure_dir(output_json_path)\n\nwith open(output_json_path, 'w') as json_file:\n    json.dump(final_structure, json_file, indent=4)\n\nprint(f\"Interaction graph created and saved as JSON at {output_json_path}\")\n\n../data/multilayer_network/raw/BIOGRID-ORGANISM-4.4.231.tab3.zip: 169MB [00:11, 15.1MB/s] \n\n\nInteraction graph created and saved as JSON at ../data/multilayer_network/processed/BioGRID_interactions.23-09-2024.json",
    "crumbs": [
      "Downloading biological data"
    ]
  },
  {
    "objectID": "download_data.html#kegg-drugs",
    "href": "download_data.html#kegg-drugs",
    "title": "Downloading biological data",
    "section": "2. KEGG drugs",
    "text": "2. KEGG drugs\n\ndef download_file_with_progress(url, filename):\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        file_size = int(r.headers.get('content-length', 0))\n        chunk_size = max(4096, file_size // 100)\n        with open(filename, 'wb') as file_out, tqdm(\n            total=file_size, unit='B', unit_scale=True, desc=str(filename)\n        ) as pbar:\n            for chunk in r.iter_content(chunk_size=chunk_size):\n                file_out.write(chunk)\n                pbar.update(len(chunk))\n\ndate = datetime.now().strftime(\"%d-%m-%Y\")\nrelease = 'br08310'\nurl = f'https://www.genome.jp/kegg-bin/download_htext?htext={release}.keg&format=json&filedir='\n\ndata_dir = Path('../data/multilayer_network/raw')\ndata_dir.mkdir(parents=True, exist_ok=True)\ndownload_file_with_progress(url, data_dir / f'{release}.json')\n\ndef extract_drugs_and_receptors(node, drug_to_receptors):\n    if 'children' in node:\n        if 'name' in node and '[' in node['name']:\n            receptor_name = re.sub(r'\\s*\\(.*?\\)\\s*', '', node['name'].split('[')[0].strip()).replace('*', '')\n            for drug in node.get('children', []):\n                drug_id, _, drug_name = drug['name'].partition(' ')\n                drug_name = drug_name.lstrip()\n                drug_to_receptors.setdefault(drug_id, {'receptors': [], 'drug_name': drug_name}).get('receptors').append(receptor_name)\n        else:\n            for child in node.get('children', []):\n                extract_drugs_and_receptors(child, drug_to_receptors)\n\ndrug_to_receptors = {}\nextract_drugs_and_receptors(json.loads((data_dir / f'{release}.json').read_text()), drug_to_receptors)\n\noutput = {\n    \"nodes\": [\n        {\"id\": receptor, \"label\": receptor, \"type\": \"protein\"} for receptor in {receptor for info in drug_to_receptors.values() for receptor in info['receptors']}\n    ],\n    \"edges\": [\n        {\"source\": src, \"target\": tgt, \"attributes\": {\"Drug ID\": drug_id, \"Drug Name\": info['drug_name']}}\n        for drug_id, info in drug_to_receptors.items() for src, tgt in combinations(set(info['receptors']), 2)\n    ]\n}\n\noutput_path = Path('../data/multilayer_network/processed') / f'KEGG_drugs.{date}.json'\noutput_path.parent.mkdir(parents=True, exist_ok=True)\noutput_path.write_text(json.dumps(output, indent=4))\n\nprint(f\"JSON file created at {output_path}\")\n\n../data/multilayer_network/raw/br08310.json: 675kB [00:03, 207kB/s] \n\n\nJSON file created at ../data/multilayer_network/processed/KEGG_drugs.23-09-2024.json",
    "crumbs": [
      "Downloading biological data"
    ]
  },
  {
    "objectID": "download_data.html#mondo-diseases",
    "href": "download_data.html#mondo-diseases",
    "title": "Downloading biological data",
    "section": "3. MONDO diseases",
    "text": "3. MONDO diseases\nURL address http://purl.obolibrary.org/obo/mondo.owl contains a 215 MB ontology, however the cell below doesn’t create any processed dataset. Changed schema?\n\ndef download_file(url, filename):\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        total_size_in_bytes = int(r.headers.get('content-length', 0))\n        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, desc=\"Downloading ontology\")\n        with open(filename, 'wb') as file:\n            for chunk in r.iter_content(chunk_size=1024):\n                progress_bar.update(file.write(chunk))\n        progress_bar.close()\n\ndef extract_mondo2omim(g):\n    rdfl = rdflib.Namespace('http://www.w3.org/2000/01/rdf-schema#')\n    owl = rdflib.Namespace('http://www.w3.org/2002/07/owl#')\n    oboInOwl = rdflib.Namespace('http://www.geneontology.org/formats/oboInOwl#')\n    mondo2omim = {}\n    mondo2descr = {}\n\n    for subj in tqdm(g.subjects(), desc=\"Extracting subjects\"):\n        for i in g.triples((subj, owl['annotatedSource'], None)):\n            mondo = i[2]\n            mondoID = mondo.toPython().rsplit('/', 1)[-1]\n            for j in g.triples((mondo, rdfl['label'], None)):\n                descr = j[2].toPython()\n                mondo2descr[mondoID] = descr\n            for w in g.triples((mondo, oboInOwl['hasDbXref'], None)):\n                if w[2].startswith(rdflib.Literal('OMIM:')):\n                    omim = w[2].toPython()\n                    mondo2omim[mondoID] = omim\n    return mondo2omim, mondo2descr\n\ndef fetch_genes(url):\n    try:\n        df = pd.read_csv(url, sep='\\t', header=0).dropna(subset=['evidence'])\n        df = df[df['subject_taxon_label'].str.contains('Homo sapiens', na=False)]\n        genes = df[df['evidence'].str.contains('ECO:0000220')].subject_label.unique().tolist()\n        return genes\n    except Exception as e:\n        print(f\"Failed to fetch genes from URL: {url} due to {e}\")\n        return []\n\ndef generate_json(mondo2genes, mondo2descr, output_file):\n    nodes = [{\"id\": gene, \"label\": gene, \"type\": \"gene\"} for gene_list in mondo2genes.values() for gene in gene_list]\n    edges = [{\"source\": gene_pair[0], \"target\": gene_pair[1], \"attributes\": {\"MONDO Identifier\": mondo, \"Description\": mondo2descr[mondo]}} \n             for mondo, genes in mondo2genes.items() for gene_pair in combinations(genes, 2)]\n\n    final_json = {\"nodes\": list({v['id']:v for v in nodes}.values()), \"edges\": edges}\n    with open(output_file, 'w') as json_file:\n        json.dump(final_json, json_file, indent=4)\n\nurl = 'http://purl.obolibrary.org/obo/mondo.owl'\nfilename = '../data/multilayer_network/raw/mondo.owl'\noutput_path = Path(\"../data/multilayer_network/processed/MoNDO_diseases.\" + datetime.now().strftime(\"%d-%m-%Y\") + \".json\")\n\n\ndownload_file(url, filename)\ng = rdflib.Graph()\n# g.load(filename)\ng.parse(filename)\nmondo2omim, mondo2descr = extract_mondo2omim(g)\n    \ninputs = [f\"https://solr.monarchinitiative.org/solr/golr/select/?defType=edismax&qt=standard&indent=on&wt=csv&rows=100000&start=0&fl=subject,subject_label,subject_taxon,subject_taxon_label,object,object_label,relation,relation_label,evidence,evidence_label,source,is_defined_by,qualifier&facet=true&facet.mincount=1&facet.sort=count&json.nl=arrarr&facet.limit=25&facet.method=enum&csv.encapsulator=%22&csv.separator=%09&csv.header=true&csv.mv.separator=%7C&fq=subject_category:%22gene%22&fq=object_category:%22disease%22&fq=object_closure:%22{mondo.replace('_', ':')}%22&facet.field=subject_taxon_label&q=*:*\" for mondo in mondo2omim.keys()]\nwith multiprocessing.Pool() as pool:\n    genes_list = list(tqdm(pool.imap(fetch_genes, inputs), total=len(inputs), desc=\"Fetching genes\"))\n    \nmondo2genes = {mondo: genes for mondo, genes in zip(mondo2omim.keys(), genes_list) if genes}\ngenerate_json(mondo2genes, mondo2descr, output_path)\nprint(f\"Generated JSON file at {output_path}\")",
    "crumbs": [
      "Downloading biological data"
    ]
  },
  {
    "objectID": "download_data.html#reactome-pathways",
    "href": "download_data.html#reactome-pathways",
    "title": "Downloading biological data",
    "section": "4. Reactome pathways",
    "text": "4. Reactome pathways\n\nnow = datetime.now()\n\n# Download Uniprot id mapping\nurl = \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/by_organism/HUMAN_9606_idmapping.dat.gz\"\nresponse = requests.get(url)\nfilename = 'data/multilayer_network/raw/HUMAN_9606_idmapping.dat.gz'\noutname = 'data/multilayer_network/raw/HUMAN_9606_idmapping.dat'\nwith open(filename, 'wb') as file:\n    file.write(response.content)\nwith gzip.open(filename, 'rb') as f_in:\n    with open(outname, 'wb') as f_out:\n        shutil.copyfileobj(f_in, f_out)\nos.remove(filename)\nmapping_dict = {}\nwith open(outname, 'r') as file:\n    for line in file:\n        parts = line.strip().split()\n        uni_id, identifier_type, value = parts[0], parts[1], ' '.join(parts[2:])\n        if identifier_type == 'Gene_Name':\n            mapping_dict[uni_id] = value\n\n# Download Reactome lowest levels\nurl = 'https://reactome.org/download/current/UniProt2Reactome.txt'\nresponse = requests.get(url)\nfilename = 'data/multilayer_network/raw/UniProt2Reactome.txt'\nwith open(filename, 'wb') as file:\n    file.write(response.content)\n\nnodes = []\nedges = []\n\nreactome = {}\ndescr_dict = {}\nwith open(filename, 'r') as file:\n    for line in file:\n        line = line.strip()\n        parts = line.split('\\t')\n        if len(parts) &lt; 6:\n            continue  # Skip incomplete lines\n        prot, pathway, descr, ec, taxo = parts[0], parts[1], parts[3], parts[4], parts[5]\n        descr_dict[pathway] = descr\n        if taxo != \"Homo sapiens\":\n            continue\n        # Skip certain evidence codes if needed\n        # if ec not in [\"EXP\", \"IDA\", \"IPI\", \"IMP\", \"IGI\", \"IEP\"]:\n        #    continue\n        if pathway not in reactome:\n            reactome[pathway] = []\n        if prot not in reactome[pathway]:\n            try:\n                gene = mapping_dict[prot]\n            except:\n                gene = prot\n            reactome[pathway].append(gene)\n        if gene not in [node['id'] for node in nodes]:\n            nodes.append({\n                \"id\": gene,\n                \"label\": gene,\n                \"type\": \"protein\"\n            })\n\nfor k,v in reactome.items():\n    if len(v) &gt; 1:\n        for gene_pair in combinations(v, 2):\n            edges.append({\n                \"source\": gene_pair[0],\n                \"target\": gene_pair[1],\n                \"attributes\": {\n                    \"Reactome Identifier\": k,\n                    \"Description\": descr_dict[k]\n                }\n            })\n\nfinal_json = {\"nodes\": nodes, \"edges\": edges}\nfile_name = \"data/multilayer_network/processed/Reactome_pathways.\" + now.strftime(\"%d-%m-%Y\") + \".json\"\nwith open(file_name, 'w') as json_file:\n    json.dump(final_json, json_file, indent=4)",
    "crumbs": [
      "Downloading biological data"
    ]
  },
  {
    "objectID": "download_data.html#recon3d-metabolites",
    "href": "download_data.html#recon3d-metabolites",
    "title": "Downloading biological data",
    "section": "5. Recon3D metabolites",
    "text": "5. Recon3D metabolites\n\n# Use the current timestamp for naming the output file later\nnow = datetime.now().strftime(\"%d-%m-%Y\")\n\n# Function to download and decompress the model file\ndef download_and_decompress(url, output_path):\n    with urllib.request.urlopen(url) as response, open(output_path, 'wb') as out_file:\n        out_file.write(gzip.decompress(response.read()))\n\n# Define URLs and file paths\nmodel_url = 'http://bigg.ucsd.edu/static/models/Recon3D.xml.gz'\nmodel_file_path = 'data/multilayer_network/raw/Recon3D.xml'\nmetabolites_file_path = 'src/metabolites_to_prune.txt'\n\n# Download and import the Recon3D model\ndownload_and_decompress(model_url, model_file_path)\nmodel = cobra.io.read_sbml_model(model_file_path)\n\n# Import the metabolites to prune\nwith open(metabolites_file_path) as f:\n    remove_metabolites = [line.split(' ')[0].strip() for line in f]\n\n# Function to process metabolites and genes\ndef process_metabolites_and_genes(model, remove_metabolites):\n    products_dict, reactants_dict, metabo_name_dict = defaultdict(list), defaultdict(list), {}\n    gene_pattern, metabo_pattern = r'__[clmerginx]$', r'__\\d+__\\d+$'\n\n    for reaction in model.reactions:\n        for gene in filter(lambda g: g.id.split('_')[0] != \"0\", reaction.genes):\n            gene_id = gene.id.split('_')[0]\n            for metabolite_group, metabolites in (('products', reaction.products), ('reactants', reaction.reactants)):\n                for metabolite in metabolites:\n                    metabo_id = re.sub(gene_pattern, '', metabolite.id)\n                    metabo_name_dict[metabo_id] = metabolite.name\n                    if gene_id not in (products_dict if metabolite_group == 'products' else reactants_dict)[metabo_id]:\n                        (products_dict if metabolite_group == 'products' else reactants_dict)[metabo_id].append(gene.name)\n\n    metabolites_dict = defaultdict(list)\n    for metabo_id, genes in {**products_dict, **reactants_dict}.items():\n        if metabo_id not in remove_metabolites:\n            metabolites_dict[re.sub(metabo_pattern, '', metabo_id)].extend(set(genes))\n\n    return {k: list(set(v)) for k, v in metabolites_dict.items()}, metabo_name_dict\n\nmetabolites_dict, metabo_name_dict = process_metabolites_and_genes(model, remove_metabolites)\n\n# Preparing data for JSON output\nnodes = [{\"id\": gene, \"label\": gene, \"type\": \"protein\"} for gene in set(gene for genes in metabolites_dict.values() for gene in genes)]\nedges = [{\n    \"source\": gene_pair[0],\n    \"target\": gene_pair[1],\n    \"attributes\": {\"Metabolite\": metabo, \"Descriptive name\": metabo_name_dict[metabo]}\n} for metabo, genes in metabolites_dict.items() for gene_pair in combinations(genes, 2)]\n\n# Saving the graph to a JSON file\noutput_file_path = f'data/multilayer_network/processed/Recon3D_metabolites.{now}.json'\nwith open(output_file_path, 'w') as f:\n    json.dump({\"nodes\": nodes, \"edges\": edges}, f, indent=4)\n\nprint(f\"Graph saved to {output_file_path}\")",
    "crumbs": [
      "Downloading biological data"
    ]
  }
]